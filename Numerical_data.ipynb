{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification supervisée des données numériques (quantitatives)\n",
    "Dans ce lab, nous allons voir comment réaliser la tàche de la classification supervisée en utilisant Python + Jupyter Notebook avec les librairies sklearn et keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. importation des librairies\n",
    "Avec Pandas on peut manipuler lire et écrire nos jeux de données, généralement avec une extension .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # lire et manipuler les IO .csv\n",
    "#import numpy as np   \n",
    "#import matplotlib \n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.model_selection import train_test_split #découper l'ensemble de train et de test de maniere aléatoire\n",
    "#import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importation des données\n",
    "Avec la fonction read_csv de Pandas: on peut mettre dans notre dataframe le contenu du fichier csv, en indiquant comme paramètre (1: le chemin ou la source où se trouve le fichier csv, 2: les séparateurs entre les valeurs dans notre cas ces des vergules) en troisième position, un paramètre facultatif pour spécifier le type d'encodage de notre fichier exemple encoding =\"UTF8\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>Age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  Age  \\\n",
       "0   1                 1                              0.766127   45   \n",
       "1   2                 0                              0.957151   40   \n",
       "2   3                 0                              0.658180   38   \n",
       "3   4                 0                              0.233810   30   \n",
       "4   5                 0                              0.907239   49   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                     2   0.802982         9120.0   \n",
       "1                                     0   0.121876         2600.0   \n",
       "2                                     1   0.085113         3042.0   \n",
       "3                                     0   0.036050         3300.0   \n",
       "4                                     1   0.024926        63588.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               13                        0   \n",
       "1                                4                        0   \n",
       "2                                2                        1   \n",
       "3                                5                        0   \n",
       "4                                7                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             6                                     0   \n",
       "1                             0                                     0   \n",
       "2                             0                                     0   \n",
       "3                             0                                     0   \n",
       "4                             1                                     0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/home/notebooks/Amine/Bank DataSets Experimentations/Give me some credit.csv', sep=',')#lire le fichier csv \n",
    "df.head()  # afficher les 5 premieres lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistiques descriptives élémentaires\n",
    "\n",
    "Lire les informations sur nos données (Types d'attributs, valeurs manquantes...)\n",
    "Pandas nous permet de voir les informations sur notre benchmark\n",
    "exemple: avec dataframe.info() il nous affiche tout les attributs de notre fichier avec le type de donnée et le nombre de valeurs de chaque colonne\n",
    "\n",
    "dataframe.columns permet de citer les noms de toutes les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 12 columns):\n",
      "ID                                      150000 non-null int64\n",
      "SeriousDlqin2yrs                        150000 non-null int64\n",
      "RevolvingUtilizationOfUnsecuredLines    150000 non-null float64\n",
      "Age                                     150000 non-null int64\n",
      "NumberOfTime30-59DaysPastDueNotWorse    150000 non-null int64\n",
      "DebtRatio                               150000 non-null float64\n",
      "MonthlyIncome                           120269 non-null float64\n",
      "NumberOfOpenCreditLinesAndLoans         150000 non-null int64\n",
      "NumberOfTimes90DaysLate                 150000 non-null int64\n",
      "NumberRealEstateLoansOrLines            150000 non-null int64\n",
      "NumberOfTime60-89DaysPastDueNotWorse    150000 non-null int64\n",
      "NumberOfDependents                      146076 non-null float64\n",
      "dtypes: float64(4), int64(8)\n",
      "memory usage: 13.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info() #donner les infos de notre data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'SeriousDlqin2yrs', 'RevolvingUtilizationOfUnsecuredLines', 'Age',\n",
       "       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n",
       "       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n",
       "       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
       "       'NumberOfDependents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # citer les colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. préparation des données\n",
    "Dans cette étape nous déterminons les attributs choisis pour l'entrainement et nous définissons l'attribut \"classe\" de notre benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir les attraibuts qui nous intéréssent, ici j'ai éliminé les attributs qui contiennent des nulles\n",
    "df_features = df[['ID', 'RevolvingUtilizationOfUnsecuredLines', 'Age',\n",
    "       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', #'MonthlyIncome',\n",
    "       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n",
    "       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "       #'NumberOfDependents'\n",
    "                 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir l'attribut classe\n",
    "df_labels = df[['SeriousDlqin2yrs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on veut schématiser la distribution de nos classes, il suffit de faire appel à la libraire seaborn, en suite définir l'attribut concerné "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff847c0b4e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# schématiser la distribution des classes\n",
    "sns.countplot(df['SeriousDlqin2yrs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diviser le dataset en données d'entrainement et données de teste\n",
    "Ceci est réalisable avec sklearn qui permet de prendre aléatoirement des données de teste à partir du benchmark et laisser le reste pour l'apprentissage.\n",
    "La fonction train_test_split(param1,param2,param3,param4) prends 4 paramétres:\n",
    "le premier dédié à l'ensemble d'entrainement, le deuxième à l'ensemble de teste, le troisième c'est le paramètre du % de l'ensemble de test (généralement entre 15 et 40%), \n",
    "\n",
    "le 4 ème paramétre (facultatif) pour spécifier quel type de fonction random utiliser:\n",
    "si vous utilisez random_state = some_number, vous pouvez garantir que la sortie de Run 1 sera égale à la sortie de Run 2, c'est-à-dire que votre split sera toujours le même. Peu importe ce que le nombre réel random_state est 42, 0, 21, ... L'important est que chaque fois que vous utilisez 42, vous obtiendrez toujours la même sortie la première fois que vous faites la division. Ceci est utile si vous voulez des résultats reproductibles, par exemple dans la documentation, afin que tout le monde puisse toujours voir les mêmes nombres lors de l'exécution des exemples.\n",
    "\n",
    "Cette fonction retourne 4 sorties: \n",
    "La 1ere est le sous-ensembles aléatoires d'entrainement \n",
    "La 2éme est le vecteur de leurs labels (leurs classes).\n",
    "La 3ème est le sous-ensemble aléatoire pour le teste.\n",
    "La 4ème est le vecteur de leurs labels (leurs classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#decouper le data set en 30% pour test et 70% pour train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".shape permet de savoir la dimension d'un ensemble.\n",
    "Par exemple ici l'ensemble d'entrainement est composé de 105000 lignes et 9 colonnes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (105000, 9)\n",
      "x_test shape: (45000, 9)\n",
      "y_train shape: (105000, 1)\n",
      "y_test shape: (45000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', X_train.shape) # .shape permet de voir la\n",
    "print('x_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Appliquer les Algorithmes de la fouille de données \n",
    "### sklearn : \n",
    "Contient la plus part des algorithmes de data mining , ici par exemple on fait appel aux algorithmes Naive Bayes , Support Vector Machines, et au KNN (algorithme des k plus proches voisins) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## DATA Mining Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score #confusion_matrix\n",
    "#import matplotlib.pyplot as plt \n",
    "#import itertools    #pour dessiner la matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  0.04556199999999988\n",
      "Bernoulli accuracy score\n",
      "0.9266666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#1 Bernouli Naive Bayes\n",
    "import time\n",
    "bnb= BernoulliNB()\n",
    "tps1 = time.clock()\n",
    "#fitting\n",
    "bnb.fit(X_train, y_train) #.values.ravel()\n",
    "# y = ravel(a) : If a is a matrix, y is a 1-D ndarray, otherwise y is an array of the same subtype as a. \n",
    "#The shape of the returned array is (a.size,). \n",
    "#Matrices are special cased for backward compatibility.\n",
    "\n",
    "ypredBnb = bnb.predict(X_test)\n",
    "tps2 = time.clock() \n",
    "ts = tps2 - tps1\n",
    "print(\"time = \",ts)\n",
    "print ('Bernoulli accuracy score')\n",
    "print (accuracy_score(y_test, ypredBnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.021079000000000292\n",
      "Multinomial accuracy score\n",
      "0.26982222222222224\n"
     ]
    }
   ],
   "source": [
    "#2 Multinomial Naive Bayes\n",
    "mnb=MultinomialNB()\n",
    "#fitting\n",
    "tpp1= time.clock()\n",
    "mnb.fit(X_train, y_train.values.ravel())\n",
    "ypredMnb = mnb.predict(X_test)\n",
    "tpp2= time.clock()\n",
    "tp=tpp2-tpp1\n",
    "print('time =' ,tp)\n",
    "print ('Multinomial accuracy score')\n",
    "print (accuracy_score(y_test, ypredMnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 SVMM\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 8.861974\n",
      "Linear Svm accuracy score\n",
      "0.9211555555555555\n"
     ]
    }
   ],
   "source": [
    "lsvm= LinearSVC()\n",
    "\n",
    "tp1=time.clock()\n",
    "#fitting\n",
    "lsvm.fit(X_train, y_train.values.ravel())\n",
    "#ypred\n",
    "ypredLsvm = lsvm.predict(X_test)\n",
    "tp2=time.clock()\n",
    "tp=tp2-tp1\n",
    "print(\"time =\" , tp)\n",
    "print ('Linear Svm accuracy score')\n",
    "print (accuracy_score(y_test, ypredLsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 KNN Uniform\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 1.1546389999999995\n",
      "KNN Uniform accuracy score\n",
      "0.9337777777777778\n"
     ]
    }
   ],
   "source": [
    "#UNIFORM KNN\n",
    "knn_uni = KNeighborsClassifier(n_neighbors=13, weights = 'uniform')\n",
    "#fitting\n",
    "tt1=time.clock()\n",
    "knn_uni.fit(X_train, y_train.values.ravel())\n",
    "#ypred\n",
    "ypred_knn_uni = knn_uni.predict(X_test)\n",
    "tt2=time.clock()\n",
    "tt=tt2 - tt1\n",
    "print(\"time =\",tt )\n",
    "print ('KNN Uniform accuracy score')\n",
    "print (accuracy_score(y_test, ypred_knn_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 1.1826849999999993\n",
      "Distant accuracy score\n",
      "0.9335777777777777\n"
     ]
    }
   ],
   "source": [
    "# 5 KNN Distant\n",
    "knn_dist = KNeighborsClassifier(n_neighbors=13, weights = 'distance')\n",
    "#fitting\n",
    "tt1 = time.clock()\n",
    "knn_dist.fit(X_train, y_train.values.ravel())\n",
    "#ypred\n",
    "ypred_knn_dist = knn_dist.predict(X_test)\n",
    "tt2=time.clock()\n",
    "tt=tt2 - tt1\n",
    "print(\"time =\",tt )\n",
    "print ('Distant accuracy score')\n",
    "print (accuracy_score(y_test, ypred_knn_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Appliquer les réseaux de neuronnes pour la classification des données\n",
    "En utilisant KERAS on peut définir notre modèle : en spécifiant le nombre de couches,le nombre de neuronnes dans chaque couche, et la fonction d'activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Machine Learning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32#32\n",
    "epochs = 15\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = [ModelCheckpoint(filepath='modNN_.hdf5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94500 samples, validate on 10500 samples\n",
      "Epoch 1/15\n",
      "94500/94500 [==============================] - 30s 312us/step - loss: 1.0939 - acc: 0.9321 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 2/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 3/15\n",
      "94500/94500 [==============================] - 11s 121us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 4/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 5/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 6/15\n",
      "94500/94500 [==============================] - 12s 123us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 7/15\n",
      "94500/94500 [==============================] - 12s 123us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 8/15\n",
      "94500/94500 [==============================] - 12s 123us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 9/15\n",
      "94500/94500 [==============================] - 12s 123us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 10/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 11/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 12/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 13/15\n",
      "94500/94500 [==============================] - 12s 122us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 14/15\n",
      "94500/94500 [==============================] - 12s 123us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n",
      "Epoch 15/15\n",
      "94500/94500 [==============================] - 12s 123us/step - loss: 1.0846 - acc: 0.9327 - val_loss: 1.0546 - val_acc: 0.9346\n"
     ]
    }
   ],
   "source": [
    "# model.fit trains the model\n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "# You can see the validation loss decreasing slowly when you run this\n",
    "# Because val_loss is no longer decreasing we stop training to preventoverfitting\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "#import sys\n",
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "#from datetime import datetime\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "\n",
    "#device_name=\"/gpu:0\"\n",
    "\n",
    "#device_name=\"/device:CPU:0\"   # UTILISATION DU GPU #4\n",
    "\n",
    "\n",
    "#with tf.device(device_name):\n",
    "model = Sequential()\n",
    "model.add(Dense(514, input_shape=(9,)))#max_words, X_train.shape[1:]\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(256 ))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128 ))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "    #model.add(Dense(num_classes,))#num_classes\n",
    "    #model.add(Activation('softmax'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',#sparse_categorical_crossentropy\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=checkpoint, # Checkpoint\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 2s 40us/step\n",
      "Test accuracy: 0.9337777777777778\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=32, verbose=1)\n",
    "\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D,Convolution2D,Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 94500 samples, validate on 10500 samples\n",
      "Epoch 1/15\n",
      "94500/94500 [==============================] - 25s 262us/step - loss: 0.2437 - acc: 0.9335 - val_loss: 0.2466 - val_acc: 0.9303\n",
      "Epoch 2/15\n",
      "94500/94500 [==============================] - 22s 235us/step - loss: 0.2348 - acc: 0.9335 - val_loss: 0.2426 - val_acc: 0.9303\n",
      "Epoch 3/15\n",
      "94500/94500 [==============================] - 22s 233us/step - loss: 0.2333 - acc: 0.9336 - val_loss: 0.2464 - val_acc: 0.9303\n",
      "Epoch 4/15\n",
      "94500/94500 [==============================] - 22s 232us/step - loss: 0.2319 - acc: 0.9337 - val_loss: 0.2492 - val_acc: 0.9304\n",
      "Epoch 5/15\n",
      "94500/94500 [==============================] - 22s 231us/step - loss: 0.2319 - acc: 0.9337 - val_loss: 0.2599 - val_acc: 0.9303\n",
      "Epoch 6/15\n",
      "94500/94500 [==============================] - 21s 227us/step - loss: 0.2313 - acc: 0.9338 - val_loss: 0.2443 - val_acc: 0.9303\n",
      "Epoch 7/15\n",
      "94500/94500 [==============================] - 22s 228us/step - loss: 0.2314 - acc: 0.9336 - val_loss: 0.2441 - val_acc: 0.9306\n",
      "Epoch 8/15\n",
      "94500/94500 [==============================] - 22s 229us/step - loss: 0.2307 - acc: 0.9336 - val_loss: 0.2523 - val_acc: 0.9297\n",
      "Epoch 9/15\n",
      "94500/94500 [==============================] - 21s 224us/step - loss: 0.2306 - acc: 0.9337 - val_loss: 0.2636 - val_acc: 0.9296\n",
      "Epoch 10/15\n",
      "94500/94500 [==============================] - 21s 224us/step - loss: 0.2303 - acc: 0.9337 - val_loss: 0.2467 - val_acc: 0.9300\n",
      "Epoch 11/15\n",
      "94500/94500 [==============================] - 22s 229us/step - loss: 0.2282 - acc: 0.9341 - val_loss: 0.2677 - val_acc: 0.9305\n",
      "Epoch 12/15\n",
      "94500/94500 [==============================] - 21s 227us/step - loss: 0.2284 - acc: 0.9339 - val_loss: 0.2448 - val_acc: 0.9307\n",
      "Epoch 13/15\n",
      "94500/94500 [==============================] - 21s 225us/step - loss: 0.2275 - acc: 0.9340 - val_loss: 0.2567 - val_acc: 0.9303\n",
      "Epoch 14/15\n",
      "94500/94500 [==============================] - 21s 223us/step - loss: 0.2277 - acc: 0.9339 - val_loss: 0.2493 - val_acc: 0.9307\n",
      "Epoch 15/15\n",
      "94500/94500 [==============================] - 21s 224us/step - loss: 0.2269 - acc: 0.9340 - val_loss: 0.2603 - val_acc: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7656cebac8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "#def create_model():\n",
    "# ValueError: Error when checking model input: expected embedding_2_input to have shape (None, 300) but got array with shape (33908, 42)\n",
    "model = Sequential()\n",
    "model.add(Embedding(2, 2, input_length=9))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(2, 2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)\n",
    "\n",
    "\n",
    "#model.compile(loss='binary_crossentropy',#sparse_categorical_crossentropy\n",
    " #             optimizer='adam',\n",
    "  #            metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=checkpoint, # Checkpoint\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "#    return model\n",
    "#CNN(5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 3s 63us/step\n",
      "Test accuracy: 0.9340222222222222\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=32, verbose=1)\n",
    "#print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
